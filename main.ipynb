{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f2b28c-a8dd-4e2d-a6d4-cd7225b2685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c802b-10e2-425a-966b-550df3e4cce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7fe2d6ec434d99877431ef69788cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e816bf66ff4dd9bbbfe80a05395989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# „É≠„Éº„Ç´„É´Áí∞Â¢É„Åã„Çâ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Ç§„É≥„Éù„Éº„Éà\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'text',\n",
    "    data_files = {\n",
    "        \"train\": \"train.txt\",\n",
    "        \"validation\": \"validation.txt\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88595a5c-be9f-45a3-95ab-3ff1941b24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cyberagent/calm2-7b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee940aab-b8c0-4389-a034-c4692d4647f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec754401-6795-44bf-ac67-7cf6328d4299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a189afd2ff49f8ad865b31587612e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad59f66c14294ae5a2acdd273cc91a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698703639a104ccda3922abd328252a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_false = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c930c4a-1f29-4c32-ab94-2ca1e397cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.27 ms, sys: 12.2 ms, total: 19.5 ms\n",
      "Wall time: 8.65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = tokenizer_false(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889c9d34-b776-4259-93fe-aa5255d8b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30103db2-4898-4814-8fa4-ae749aa7843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.39 ms, sys: 1.02 ms, total: 7.41 ms\n",
      "Wall time: 5.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b = tokenizer(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a839b-d500-44b5-9046-85011e37e0bf",
   "metadata": {},
   "source": [
    "‚Üë\n",
    "„Çπ„Éî„Éº„ÉâÊØîËºÉ\n",
    "\n",
    "use_fast = True„ÅÆÊñπ„ÅåÂÉÖ„Åã„Å´ÈÄü„ÅÑÔºüÔºü\n",
    "\n",
    "use_fast„Å´„Åô„Çã„Åì„Å®„ÅßÊ≠£Á¢∫ÊÄß„ÇíÊ¨†„ÅÑ„Åü„Å®„ÅÑ„ÅÜÂ†±Âëä„ÇÇ„ÅÇ„ÇäÊ≥®ÊÑèÔºà[ÂèÇËÄÉ](https://github.com/huggingface/text-generation-inference/issues/469)Ôºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff11655-0282-42c9-b410-74b1cd02d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÂêÑ„Éá„Éº„ÇøÂçò‰Ωç„ÅÆ„Éà„Éº„ÇØ„É≥Âåñ\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e473ef-a047-458c-932b-f881fb6b8d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a4e30efdc04dddafe01f0e897b2e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/86 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0e6a035f284ebe83a661e0241265cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True, # „Éà„Éº„ÇØ„É≥Âåñ„ÅÆÂá¶ÁêÜ„Å´Ë§áÊï∞„Éê„ÉÉ„ÉÅ‰Ωø„ÅÜ\n",
    "    num_proc = 4, # ‰∏¶Ë°å„Éó„É≠„Çª„ÇπÊï∞\n",
    "    remove_columns = ['text'] # text„Éá„Éº„Çø„ÅØÁî®Ê∏à„Åø„Å™„ÅÆ„ÅßÂâäÈô§\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3e2cf-d8ff-492c-9a13-f198f5efd98d",
   "metadata": {},
   "source": [
    "#### batched: „Éê„ÉÉ„ÉÅ„Çí‰Ωø„Å£„Åü„Éà„Éº„ÇØ„É≥Âåñ\n",
    "\n",
    "„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÁâπÂÆö„ÅÆ„É°„É¢„É™„Çµ„Ç§„Ç∫„Å´ËêΩ„Å®„ÅóËæº„ÇÄ = Èï∑„ÅÑÊñáÁ´†„ÇíÁü≠„ÅèÂàÜÂâ≤„Åô„Çã\n",
    "\n",
    "Â§ß„Åç„ÇÅ„ÅÆ„Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„Çí‰Ωø„ÅÜ„Å®Âá¶ÁêÜ„ÅåÊó©„ÅèÁµÇ„Çè„ÇãÂÇæÂêë\n",
    "\n",
    "„Åì„Åì„Åß„É°„É¢„É™„Çµ„Ç§„Ç∫„Å®„ÅØ„É°„É¢„É™„ÇíÊåá„Åô„Åì„Å®„ÇÇ„ÅÇ„Çã„Åó„ÄÅGPU„ÇíÊåá„Åô„Åì„Å®„ÇÇ„ÅÇ„Çã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05c30d-46a6-4141-aa82-93909196943a",
   "metadata": {},
   "source": [
    "#### num_proc: „Éû„É´„ÉÅ„Ç≥„Ç¢Áí∞Â¢É„ÅÆÂ†¥Âêà\n",
    "\n",
    "„Éû„É´„ÉÅ„Ç≥„Ç¢Áí∞Â¢É = 1„Å§„ÅÆCPU„ÅÆ‰∏≠„Å´Ë§áÊï∞„ÅÆÂçò‰ΩçCPU„Åå„ÅÇ„Çã„Ç≥„É≥„Éî„É•„Éº„Çø\n",
    "\n",
    "„Éû„É´„ÉÅ„Ç≥„Ç¢Áí∞Â¢É„ÅßÂÆüË°å„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„ÄÅÊåáÂÆö„Åó„ÅüÊï∞„ÅÆCPU„Çí‰ΩøÁî®„Åô„Çã\n",
    "\n",
    "‰∏¶Ë°åÂá¶ÁêÜ„Åô„Çã„Åì„Å®„ÅßÈ´òÈÄüÂåñ„Å´ÂØÑ‰∏é\n",
    "\n",
    "„Ç≥„Ç¢„ÅÆÊï∞„Çà„Çä„ÇÇÂ§ö„ÅèÊåáÂÆö„Åó„Å¶„Åó„Åæ„ÅÜ„Å®„ÄÅ‰∏äÈôê„Åæ„ÅßËêΩ„Å®„Åï„Çå„Çã(Ë≠¶Âëä„ÅåÂá∫„Çã)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2122544-5168-4ef3-9c76-b8578cc77de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# „Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞‰∏≠„Å´‰ΩøÁî®„Åô„Çã„É°„É¢„É™È†òÂüü„ÇíÂúßËø´„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´„ÄÅÈÅ©Âàá„Å™„Çµ„Ç§„Ç∫„Å´ÊñáÁ´†„Çí„Ç´„ÉÉ„Éà„Åô„Çã\n",
    "\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a5d36b-bbe2-437a-8504-9f58196224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(examples):\n",
    "    result = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': []\n",
    "    }\n",
    "    for ix in range(len(examples['input_ids'])):\n",
    "        # Âàá„ÇäÊç®„Å¶„ÇíËÄÉÊÖÆ„Åó„Å¶„Éá„Éº„Çø„Å´Âê´„ÇÅ„Çã„Éà„Éº„ÇØ„É≥IDs„ÅÆÊï∞\n",
    "        # „Ç´„ÉÉ„Éà„Çµ„Ç§„Ç∫„Åã„Çâ„ÅØ„ÅøÂá∫„ÇãÂàÜ„ÅÆ„Éà„Éº„ÇØ„É≥ID„ÅØÂàá„ÇäÊç®„Å¶„Å¶„Åó„Åæ„ÅÜ„ÄÅ„ÇÇ„Å£„Åü„ÅÑ„Å™„ÅÑ„Åå\n",
    "        total_length = len(examples['input_ids'][ix])\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "        for bookmark in range(0, total_length, block_size):\n",
    "            result['input_ids'].append(examples['input_ids'][ix][bookmark: bookmark + block_size])\n",
    "            result['attention_mask'].append(examples['attention_mask'][ix][bookmark: bookmark + block_size])\n",
    "    result[\"labels\"] = result['input_ids'].copy() # transformers„ÅÆ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Çà„Çälabels„Å®input_ids„ÅåÂêå„Åò„ÅßÂïèÈ°å„Å™„ÅÑ(ÂæåËø∞)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d09920-6f0f-44e7-9b83-317968ce59ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa2e3ac9fe64b3eae3d162b6874ccc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/86 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b52a21541cc419790c1594e22ba02db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    split_texts,\n",
    "    batched = True,\n",
    "    batch_size = 1000,\n",
    "    num_proc = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cdd98-f114-45c5-9fc9-d43b19e786f4",
   "metadata": {},
   "source": [
    "#### labels„Å®input_ids„ÅåÂêå„Åò„ÅßÂïèÈ°å„Å™„ÅÑ\n",
    "\n",
    "hugging-face„ÅÆ„É©„Ç§„Éñ„É©„É™„Åß„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åô„ÇãÂ†¥Âêà„ÄÅlabels„Å®input_ids„ÅåÂêå„Åò„ÅßÂïèÈ°å„Å™„ÅÑ\n",
    "\n",
    "ÂÆüÈöõ„ÅÆ„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Åß„ÅØ„ÄÅ1„Å§„ÅÆlabels„Å®input_ids„ÅÆÁµÑ„Åå„Å™„Çì„Å©„ÇÇÂÜçÂà©Áî®„Åï„Çå„Çã\n",
    "„Å®„ÅÑ„ÅÜ„ÅÆ„ÇÇÂÆüÈöõ„ÅÆ„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„Çà„Çã„ÇÇ„ÅÆ„Åß\n",
    "\n",
    "ÈÄî‰∏≠„Åæ„ÅßÁîüÊàê„Åï„Çå„ÅüÊñáÁ´†„Åã„ÇâÊ¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Çí‰∫àÊ∏¨„Åó„Å¶Á≠î„ÅàÂêà„Çè„Åõ„ÄÅ„Å®„ÅÑ„ÅÜ„ÅÆ„ÇíÊñáÁ´†„ÅÆÊúÄÂæå„Åæ„ÅßË°å„Å£„Å¶„ÅÑ„Çã„Åü„ÇÅ\n",
    "\n",
    "„Å†„Åã„Çâ‰∏Ä„Å§Ââç„ÅÆlabels„ÅåÊ¨°„ÅÆinput_ids„Å´„Å™„Çã„Å®„ÅÑ„ÅÜ„Çà„ÅÜ„Å´‰∏Ä„Å§„Åö„Å§„Åö„Çâ„Åó„Å¶„ÅÑ„Çã\n",
    "\n",
    "„Åì„Çå„Çítransformers„ÅåÊ∞ó„ÇíÂäπ„Åã„Åõ„Å¶ÂãùÊâã„Å´„ÇÑ„Å£„Å¶„Åè„Çå„Çã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f450307-bca5-467b-bb7d-a8196994f360",
   "metadata": {},
   "source": [
    "## „Åì„Åì„Åã„Çâ„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b90a16-2f8e-4295-b151-37c83e16eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d1b13c-26fe-460a-bf48-f126b8d76bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f8913b135d40bb9bf1e38edc79061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35045dda80664f23a58b863667e833ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a68a90655c94acf9c7c05d225789f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc05511962f840c3856c3341259f086f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99d86588721462c8e296fc37c592d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9951e79897b43638840ee04b66aa564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed60362ccd347fb9935d36ca9f9b5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto', # Ë§áÊï∞„ÅÆGPU„Åå„ÅÇ„Çã„Å®„Åç„Å´„ÄÅ„Åù„Çå„Çâ„ÇíÂùáÁ≠â„Å´‰ΩøÁî®„Åô„Çã\n",
    "    load_in_8bit = True # \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123b2d32-b961-4c09-87bb-15f00928c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = './results'\n",
    "evaluation_strategy = 'epoch'\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy = evaluation_strategy,\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b377fed-be43-468b-9b19-dc46f9880acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e259c3-8a45-427f-b5f8-5520bec06e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74fef6d6-24f8-46db-be43-12bd82695ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = lm_datasets[\"train\"],\n",
    "    eval_dataset = lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd297138-5328-4d5e-a139-d24b80d9711a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1217\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py:46\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m     45\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(shift_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py:28\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(source, target, ignore_index\u001b[38;5;241m=\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39mreduction)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515a3d7-c541-449f-8d6a-8631990928fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
