{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f2b28c-a8dd-4e2d-a6d4-cd7225b2685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c802b-10e2-425a-966b-550df3e4cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカル環境からデータセットをインポート\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'text',\n",
    "    data_files = {\n",
    "        \"train\": \"train.txt\",\n",
    "        \"validation\": \"validation.txt\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88595a5c-be9f-45a3-95ab-3ff1941b24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cyberagent/calm2-7b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee940aab-b8c0-4389-a034-c4692d4647f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec754401-6795-44bf-ac67-7cf6328d4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_false = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c930c4a-1f29-4c32-ab94-2ca1e397cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 ms, sys: 1.98 ms, total: 10.2 ms\n",
      "Wall time: 5.39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a = tokenizer_false(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889c9d34-b776-4259-93fe-aa5255d8b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30103db2-4898-4814-8fa4-ae749aa7843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.5 ms, sys: 1.11 ms, total: 7.6 ms\n",
      "Wall time: 4.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b = tokenizer(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a839b-d500-44b5-9046-85011e37e0bf",
   "metadata": {},
   "source": [
    "↑\n",
    "スピード比較\n",
    "\n",
    "use_fast = Trueの方が僅かに速い？？\n",
    "\n",
    "use_fastにすることで正確性を欠いたという報告もあり注意（[参考](https://github.com/huggingface/text-generation-inference/issues/469)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff11655-0282-42c9-b410-74b1cd02d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 各データ単位のトークン化\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e473ef-a047-458c-932b-f881fb6b8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True, # トークン化の処理に複数バッチ使う\n",
    "    num_proc = 4, # 並行プロセス数\n",
    "    remove_columns = ['text'] # textデータは用済みなので削除\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3e2cf-d8ff-492c-9a13-f198f5efd98d",
   "metadata": {},
   "source": [
    "#### batched: バッチを使ったトークン化\n",
    "\n",
    "データセットを特定のメモリサイズに落とし込む = 長い文章を短く分割する\n",
    "\n",
    "大きめのバッチサイズを使うと処理が早く終わる傾向\n",
    "\n",
    "ここでメモリサイズとはメモリを指すこともあるし、GPUを指すこともある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05c30d-46a6-4141-aa82-93909196943a",
   "metadata": {},
   "source": [
    "#### num_proc: マルチコア環境の場合\n",
    "\n",
    "マルチコア環境 = 1つのCPUの中に複数の単位CPUがあるコンピュータ\n",
    "\n",
    "マルチコア環境で実行している場合は、指定した数のCPUを使用する\n",
    "\n",
    "並行処理することで高速化に寄与\n",
    "\n",
    "コアの数よりも多く指定してしまうと、上限まで落とされる(警告が出る)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2122544-5168-4ef3-9c76-b8578cc77de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニング中に使用するメモリ領域を圧迫しないように、適切なサイズに文章をカットする\n",
    "\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a5d36b-bbe2-437a-8504-9f58196224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(examples):\n",
    "    result = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': []\n",
    "    }\n",
    "    for ix in range(len(examples['input_ids'])):\n",
    "        # 切り捨てを考慮してデータに含めるトークンIDsの数\n",
    "        # カットサイズからはみ出る分のトークンIDは切り捨ててしまう、もったいないが\n",
    "        total_length = len(examples['input_ids'][ix])\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "        for bookmark in range(0, total_length, block_size):\n",
    "            result['input_ids'].append(examples['input_ids'][ix][bookmark: bookmark + block_size])\n",
    "            result['attention_mask'].append(examples['attention_mask'][ix][bookmark: bookmark + block_size])\n",
    "    result[\"labels\"] = result['input_ids'].copy() # transformersのファインチューニングのアルゴリズムによりlabelsとinput_idsが同じで問題ない(後述)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d09920-6f0f-44e7-9b83-317968ce59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n"
     ]
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    split_texts,\n",
    "    batched = True,\n",
    "    batch_size = 1000,\n",
    "    num_proc = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cdd98-f114-45c5-9fc9-d43b19e786f4",
   "metadata": {},
   "source": [
    "#### labelsとinput_idsが同じで問題ない\n",
    "\n",
    "hugging-faceのライブラリでファインチューニングする場合、labelsとinput_idsが同じで問題ない\n",
    "\n",
    "実際のチューニングでは、1つのlabelsとinput_idsの組がなんども再利用される\n",
    "というのも実際のチューニングアルゴリズムによるもので\n",
    "\n",
    "途中まで生成された文章から次のトークンを予測して答え合わせ、というのを文章の最後まで行っているため\n",
    "\n",
    "だから一つ前のlabelsが次のinput_idsになるというように一つずつずらしている\n",
    "\n",
    "これをtransformersが気を効かせて勝手にやってくれる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f450307-bca5-467b-bb7d-a8196994f360",
   "metadata": {},
   "source": [
    "## ここからファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b90a16-2f8e-4295-b151-37c83e16eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextStreamer\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d1b13c-26fe-460a-bf48-f126b8d76bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fdce5edb3b4284ba1f728116d52cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = 'auto', # 複数のGPUがあるときに、それらを均等に使用する\n",
    "    load_in_8bit = True # \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defc16d1-7035-407c-8f6a-4abb73599a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt = True,\n",
    "    skip_special_tokens = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e2c20-cd8c-4b95-b76b-7ba2dd784b7a",
   "metadata": {},
   "source": [
    "#### テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2e47e-f2b3-4318-95a5-9c4030b7b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"USER:海洋調査の目的を解説してください。\\nASSISTANT:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d000451-3452-43b5-a415-6c5cd77646b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " こんにちは！お問い合わせいただきありがとうございます。どんな内容についてお問い合わせでしょうか？よろしくお願いいたします。\n",
      "USER:こんにちは？\n",
      "ASSISTANT: こんにちは！お問い合わせいただきありがとうございます。どんな内容についてお問い合わせでしょうか？よろしくお願いいたします。<|endoftext|>\n",
      "CPU times: user 4.79 s, sys: 352 ms, total: 5.15 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    message,\n",
    "    return_tensors = 'pt'\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 300,\n",
    "    do_sample = True,\n",
    "    temperature = 0.8,\n",
    "    streamer = streamer\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123b2d32-b961-4c09-87bb-15f00928c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = './results'\n",
    "evaluation_strategy = 'epoch'\n",
    "learning_rate = 2e-5\n",
    "# learning_rate = 2e-8　# 学習率を小さくしてみるも損失関数の計算はできず\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy = evaluation_strategy,\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b377fed-be43-468b-9b19-dc46f9880acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e259c3-8a45-427f-b5f8-5520bec06e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74fef6d6-24f8-46db-be43-12bd82695ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = lm_datasets[\"train\"],\n",
    "    eval_dataset = lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd297138-5328-4d5e-a139-d24b80d9711a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 28.6 s, total: 2min 28s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=12.584846742691532, metrics={'train_runtime': 149.0427, 'train_samples_per_second': 9.863, 'train_steps_per_second': 1.248, 'total_flos': 7650018506833920.0, 'train_loss': 12.584846742691532, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8515a3d7-c541-449f-8d6a-8631990928fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/integrations/peft.py:418: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./output')\n",
    "\n",
    "# you can reload self-trained model\n",
    "# AutoModelForCausalLM.from_pretrained('./output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb6987-8aed-4450-827e-4a47e9c3ff70",
   "metadata": {},
   "source": [
    "#### テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87099947-43c7-4871-b397-c4b0661e70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER3af.\n",
      "から、sfe4USER:USER:USER:USER:USER:USERRCP明堂宇 USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER：USER:USER:USER USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER_id'sdeMainooUSER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER’t,USER:USER:USER:USER:USER:USER\n",
      "USER:海洋調査の目的を解説してください。\n",
      "ASSISTANT:USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER3af.\n",
      "から、sfe4USER:USER:USER:USER:USER:USERRCP明堂宇 USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER：USER:USER:USER USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER_id'sdeMainooUSER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER：USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER:USER’t,USER:USER:USER:USER:USER:USER\n",
      "CPU times: user 52.1 s, sys: 56.9 ms, total: 52.2 s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    message,\n",
    "    return_tensors = 'pt'\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 300,\n",
    "    do_sample = True,\n",
    "    temperature = 0.8,\n",
    "    streamer = streamer\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f81f16-6558-45ae-a08d-eab1a8e6d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
