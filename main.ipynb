{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2b28c-a8dd-4e2d-a6d4-cd7225b2685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c802b-10e2-425a-966b-550df3e4cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカル環境からデータセットをインポート\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'text',\n",
    "    data_files = {\n",
    "        \"train\": \"train.txt\",\n",
    "        \"validation\": \"validation.txt\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88595a5c-be9f-45a3-95ab-3ff1941b24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cyberagent/calm2-7b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee940aab-b8c0-4389-a034-c4692d4647f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec754401-6795-44bf-ac67-7cf6328d4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_false = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c930c4a-1f29-4c32-ab94-2ca1e397cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = tokenizer_false(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c9d34-b776-4259-93fe-aa5255d8b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30103db2-4898-4814-8fa4-ae749aa7843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "b = tokenizer(datasets[\"train\"][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a839b-d500-44b5-9046-85011e37e0bf",
   "metadata": {},
   "source": [
    "↑\n",
    "スピード比較\n",
    "\n",
    "use_fast = Trueの方が僅かに速い？？\n",
    "\n",
    "use_fastにすることで正確性を欠いたという報告もあり注意（[参考](https://github.com/huggingface/text-generation-inference/issues/469)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff11655-0282-42c9-b410-74b1cd02d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 各データ単位のトークン化\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e473ef-a047-458c-932b-f881fb6b8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched = True, # トークン化の処理に複数バッチ使う\n",
    "    num_proc = 4, # 並行プロセス数\n",
    "    remove_columns = ['text'] # textデータは用済みなので削除\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3e2cf-d8ff-492c-9a13-f198f5efd98d",
   "metadata": {},
   "source": [
    "#### batched: バッチを使ったトークン化\n",
    "\n",
    "データセットを特定のメモリサイズに落とし込む = 長い文章を短く分割する\n",
    "\n",
    "大きめのバッチサイズを使うと処理が早く終わる傾向\n",
    "\n",
    "ここでメモリサイズとはメモリを指すこともあるし、GPUを指すこともある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05c30d-46a6-4141-aa82-93909196943a",
   "metadata": {},
   "source": [
    "#### num_proc: マルチコア環境の場合\n",
    "\n",
    "マルチコア環境 = 1つのCPUの中に複数の単位CPUがあるコンピュータ\n",
    "\n",
    "マルチコア環境で実行している場合は、指定した数のCPUを使用する\n",
    "\n",
    "並行処理することで高速化に寄与\n",
    "\n",
    "コアの数よりも多く指定してしまうと、上限まで落とされる(警告が出る)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2122544-5168-4ef3-9c76-b8578cc77de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニング中に使用するメモリ領域を圧迫しないように、適切なサイズに文章をカットする\n",
    "\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5d36b-bbe2-437a-8504-9f58196224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(examples):\n",
    "    result = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': []\n",
    "    }\n",
    "    for ix in range(len(examples['input_ids'])):\n",
    "        # 切り捨てを考慮してデータに含めるトークンIDsの数\n",
    "        # カットサイズからはみ出る分のトークンIDは切り捨ててしまう、もったいないが\n",
    "        total_length = len(examples['input_ids'][ix])\n",
    "        total_lenght = (total_length // block_size) * block_size\n",
    "\n",
    "        for bookmark in range(0, total_length, block_size):\n",
    "            result['input_ids'].append(examples['input_ids'][ix][bookmark: bookmark + block_size])\n",
    "            result['attention_mask'].append(examples['attention_mask'][ix][bookmark: bookmark + block_size])\n",
    "    result[\"labels\"] = result['input_ids'].copy() # transformersのファインチューニングのアルゴリズムによりlabelsとinput_idsが同じで問題ない(後述)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d09920-6f0f-44e7-9b83-317968ce59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    split_texts,\n",
    "    batched = True,\n",
    "    batch_size = 1000,\n",
    "    num_proc = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cdd98-f114-45c5-9fc9-d43b19e786f4",
   "metadata": {},
   "source": [
    "#### labelsとinput_idsが同じで問題ない\n",
    "\n",
    "hugging-faceのライブラリでファインチューニングする場合、labelsとinput_idsが同じで問題ない\n",
    "\n",
    "実際のチューニングでは、1つのlabelsとinput_idsの組がなんども再利用される\n",
    "というのも実際のチューニングアルゴリズムによるもので\n",
    "\n",
    "途中まで生成された文章から次のトークンを予測して答え合わせ、というのを文章の最後まで行っているため\n",
    "\n",
    "だから一つ前のlabelsが次のinput_idsになるというように一つずつずらしている\n",
    "\n",
    "これをtransformersが気を効かせて勝手にやってくれる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f450307-bca5-467b-bb7d-a8196994f360",
   "metadata": {},
   "source": [
    "## ここからファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b90a16-2f8e-4295-b151-37c83e16eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1b13c-26fe-460a-bf48-f126b8d76bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37902bf9-26e7-4bb4-a220-ce4d4b4d300b",
   "metadata": {},
   "source": [
    "↑\n",
    "\n",
    "なんか若干容量の厳しさがあるかも\n",
    "\n",
    "ファインチューニングと量子化の関係を調べる？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b8cc2-3b13-4175-b0f3-c1248e2551b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './results'\n",
    "evaluation_strategy = 'epoch'\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b2d32-b961-4c09-87bb-15f00928c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy = evaluation_strategy,\n",
    "    learning_rate = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fef6d6-24f8-46db-be43-12bd82695ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd297138-5328-4d5e-a139-d24b80d9711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515a3d7-c541-449f-8d6a-8631990928fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
